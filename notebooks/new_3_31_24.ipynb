{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dd1e3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from hashlib import md5\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "\n",
    "DB_FILE = 'website/events_db.json'\n",
    "EVENT_TAGS = ['exhibition',]\n",
    "\n",
    "# Create month to month number dict\n",
    "month_to_num_dict = {\n",
    "    'jan': 1,\n",
    "    'feb': 2,\n",
    "    'mar': 3,\n",
    "    'apr': 4,\n",
    "    'may': 5,\n",
    "    'jun': 6,\n",
    "    'jul': 7,\n",
    "    'aug': 8,\n",
    "    'sep': 9,\n",
    "    'oct': 10,\n",
    "    'nov': 11,\n",
    "    'dec': 12,\n",
    "    \n",
    "    'january': 1,\n",
    "    'february': 2,\n",
    "    'march': 3,\n",
    "    'april': 4,\n",
    "    'june': 6,\n",
    "    'july': 7,\n",
    "    'august': 8,\n",
    "    'september': 9,\n",
    "    'october': 10,\n",
    "    'november': 11,\n",
    "    'december': 12,\n",
    "    \n",
    "    'sept': 8,\n",
    "}\n",
    "\n",
    "def copy_json_file(source_file_path, destination_file_path):\n",
    "    \"\"\"\n",
    "    Function to take the source path of a json file and make\n",
    "    a copy of the json file to the destination_file_path.\n",
    "    \"\"\"\n",
    "    # Step 1: Open and read the JSON file\n",
    "    with open(source_file_path, 'r') as json_file:\n",
    "        # Load the JSON content into a Python data structure\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Step 2: Create or open the destination JSON file and write the data to it\n",
    "    with open(destination_file_path, 'w') as destination_json_file:\n",
    "        # Write the data to the destination file\n",
    "        json.dump(data, destination_json_file, indent=4)  # Index makes for pretty formatting\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_db():\n",
    "    \"\"\"Load the event database from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(DB_FILE, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def save_db(db):\n",
    "    \"\"\"Save the event database to a JSON file.\"\"\"\n",
    "    with open(DB_FILE, 'w') as file:\n",
    "        json.dump(db, file, indent=4, default=str)\n",
    "\n",
    "def fetch_and_parse(url):\n",
    "    \"\"\"Fetch content of a webpage and return a BeautifulSoup object.\"\"\"\n",
    "    try:\n",
    "        # Send a GET request to fetch the webpage content\n",
    "        response = requests.get(url, headers={'User-Agent': 'Your Bot 0.1'})\n",
    "        response.raise_for_status()\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_event_hash(event_details):\n",
    "    \"\"\"Generate a hash of the event details to detect changes.\"\"\"\n",
    "    event_string = json.dumps(event_details, sort_keys=True, default=str)\n",
    "    return md5(event_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "def generate_unique_identifier(event_details):\n",
    "    \"\"\"Generate a unique identifier for an event.\"\"\"\n",
    "    return f\"{event_details['name']}-{event_details['venue']}\"\n",
    "\n",
    "def process_event(event_details):\n",
    "    \"\"\"Check if an event is new or has changed and update the database accordingly.\"\"\"\n",
    "    db = load_db()\n",
    "    site_events = db.get(event_details['venue'], {})\n",
    "    \n",
    "    event_id = generate_unique_identifier(event_details)\n",
    "    event_hash = generate_event_hash(event_details)\n",
    "\n",
    "    if event_id not in site_events or site_events[event_id]['hash'] != event_hash:\n",
    "        print(f\"Updating event: {event_details['name']}\")\n",
    "        site_events[event_id] = {**event_details, 'hash': event_hash}\n",
    "        db[event_details['venue']] = site_events\n",
    "        save_db(db)\n",
    "    else:\n",
    "        print(f\"No changes detected for event: {event_details['name']}\")\n",
    "\n",
    "def scrape_de_young_and_legion_of_honor():\n",
    "    \"\"\"Scrape and process events from the de Young and Legion of Honor.\"\"\"\n",
    "    \n",
    "    def convert_date_to_dt(date_string):\n",
    "        \"\"\"Takes a date in string form and converts it to a dt object\"\"\"\n",
    "        global month_to_num_dict\n",
    "\n",
    "        date_parts = date_string.split()\n",
    "        month_num = int(month_to_num_dict[date_parts[0]])\n",
    "        day = int(date_parts[1])\n",
    "        year = int(date_parts[2])\n",
    "        if month_num and day and year:\n",
    "            date_dt = dt.date(year, month_num, day)\n",
    "        return date_dt\n",
    "\n",
    "    # Declare list of url dicts and then iterate through them\n",
    "    urls = [\n",
    "        {\n",
    "            'venue': 'de Young',\n",
    "            'base_url': 'https://www.famsf.org/calendar?type=exhibition&location=de-young'\n",
    "        },\n",
    "        {\n",
    "            'venue': 'Legion of Honor',\n",
    "            'base_url': 'https://www.famsf.org/calendar?type=exhibition&location=legion-of-honor'\n",
    "        },\n",
    "        {\n",
    "            'venue': 'Virtual',\n",
    "            'base_url': 'https://www.famsf.org/calendar?type=exhibition&location=virtual'\n",
    "        }\n",
    "    ]\n",
    "    for u in urls:\n",
    "        # Iterate through the pages\n",
    "        for i in range(1, 10):\n",
    "            if i == 1:\n",
    "                url = u['base_url']\n",
    "            else:\n",
    "                url = u['base_url'] + f\"?page={i}\"\n",
    "            soup = fetch_and_parse(url)\n",
    "            if soup:\n",
    "                # Find elements a class\n",
    "                group_elements = soup.find_all(class_=\"mt-24 xl:mt-32\")\n",
    "                \n",
    "                # If no pages left, exit loop\n",
    "                if len(group_elements) == 0: # this will be 0 when we've gone through all the pages\n",
    "                    break\n",
    "\n",
    "                for e in group_elements:\n",
    "\n",
    "                    # Extract name\n",
    "                    name = e.find(\"a\").find(\"h3\").get_text().strip()\n",
    "\n",
    "                    # Extract link\n",
    "                    link = e.find(\"a\").get(\"href\")\n",
    "\n",
    "                    # Extract date info\n",
    "                    date = e.find(class_=\"mt-12 text-secondary f-subheading-1\").get_text()\n",
    "                    \n",
    "                    # Identify phase and date fields\n",
    "                    if date.lower().split()[0] == 'through':\n",
    "                        # Get phase\n",
    "                        phase = 'current'\n",
    "                        # Get dt versions of start and end dates\n",
    "                        start_date = 'null'\n",
    "                        dates = [date.lower().replace(',', '').replace('through ', '')]\n",
    "                        end_date = convert_date_to_dt(dates[0])\n",
    "\n",
    "                    else:\n",
    "                        # Get phase\n",
    "                        phase = 'future'\n",
    "                        # Get dt versions of start and end dates\n",
    "                        dates = date.lower().replace(',', '').split(' â€“ ')\n",
    "                        # If no year in the date, add the year (use year of end date)\n",
    "                        if len(dates[0].split()) == 2:\n",
    "                            dates[0] = dates[0] + ' ' + dates[1].split()[-1]\n",
    "                        start_date = convert_date_to_dt(dates[0])\n",
    "                        end_date = convert_date_to_dt(dates[1])\n",
    "                                            \n",
    "                    event_details = {\n",
    "                        'name': name,\n",
    "                        'venue': u['venue'],\n",
    "                        'tags': ['exhibition'] + [phase], # Add phase to list of tags\n",
    "                        'phase': phase, # Possible phases are past, current, future\n",
    "                        'dates': {'start': start_date, 'end': end_date},\n",
    "                        'links': [\n",
    "                            {\n",
    "                                'link': link,\n",
    "                                'description': 'Event Page'\n",
    "                            },\n",
    "                        ],\n",
    "                    }\n",
    "                    process_event(event_details)\n",
    "\n",
    "def scrape_sfmoma():\n",
    "    \"\"\"Scrape and process events from SFMOMA.\"\"\"\n",
    "    \n",
    "    def convert_date_to_dt(date_string):\n",
    "        \"\"\"Takes a date in string form and converts it to a dt object\"\"\"\n",
    "        global month_to_num_dict\n",
    "\n",
    "        date_parts = date_string.split()\n",
    "        month_num = int(month_to_num_dict[date_parts[0]])\n",
    "        day = int(date_parts[1])\n",
    "        year = int(date_parts[2])\n",
    "        if month_num and day and year:\n",
    "            date_dt = dt.date(year, month_num, day)\n",
    "        return date_dt\n",
    "    \n",
    "    # Declare url\n",
    "    url = 'https://www.sfmoma.org/exhibitions/'\n",
    "\n",
    "    # Declare list of div ids\n",
    "    divs = [\n",
    "        {\n",
    "            'id': 'item--exhibitions-current',\n",
    "            'phase': 'current'\n",
    "        },\n",
    "        {\n",
    "            'id': 'item--exhibitions-upcoming',\n",
    "            'phase': 'future'\n",
    "        },\n",
    "        {\n",
    "            'id': 'item--exhibitions-past',\n",
    "            'phase': 'past'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Scrap info\n",
    "    soup = fetch_and_parse(url)\n",
    "    \n",
    "    # Go through and collect events in each phase (current, future, and past)\n",
    "    for phase_dict in divs:\n",
    "\n",
    "        # Assuming the structure is consistent with your provided HTML example\n",
    "        # and the container 'events' has direct children that are event elements\n",
    "        events_container = soup.find('div', id=phase_dict['id'])\n",
    "\n",
    "        # Check if events_container is found\n",
    "        if events_container:\n",
    "            # Find all direct child divs that are assumed to represent individual events\n",
    "            individual_events = events_container.find_all('a', class_='exhibitionsgrid-wrapper-grid-item')\n",
    "\n",
    "            # Iterate through events and extract details\n",
    "            for event in individual_events:\n",
    "                # Event link\n",
    "                event_link = event['href']\n",
    "\n",
    "                # Title\n",
    "                event_title = event.find(\"div\", class_=\"exhibitionsgrid-wrapper-grid-item-text-title\").text.strip()\n",
    "\n",
    "                # Floor in SFMOMA - unique to SFMOMA\n",
    "                event_floor = normalize('NFKD', event.find(\"span\", class_=\"exhibitionsgrid-wrapper-grid-item-location\").text.strip())\n",
    "\n",
    "                # Description\n",
    "                try:\n",
    "                    event_description = event.find(\"div\", class_=\"exhibitionsgrid-wrapper-grid-item-text-desc\").text.strip()\n",
    "                except AttributeError:\n",
    "                    event_description = ''\n",
    "\n",
    "                # Dates\n",
    "                event_date = event.find(\"div\", class_=\"exhibitionsgrid-wrapper-grid-item-text-date\").text.strip().lower()\n",
    "                event_date = event_date.split('member previews')[0]\n",
    "                # Replace seasons with estimated dates\n",
    "                if 'fall' in event_date:\n",
    "                    event_date = event_date.replace('fall', 'sep 20,')\n",
    "                if 'winter' in event_date:\n",
    "                    event_date = event_date.replace('winter', 'dec 20,')\n",
    "                if 'spring' in event_date:\n",
    "                    event_date = event_date.replace('spring', 'mar 20,')\n",
    "                if 'summer' in event_date:\n",
    "                    event_date = event_date.replace('summer', 'jun 20,')\n",
    "                # Get start date and end date\n",
    "                if event_date in ('new exhibition! now on view', 'ongoing'):\n",
    "                    start_date = 'null'\n",
    "                    end_date = 'null'\n",
    "                elif event_date.split()[0] == 'closing':\n",
    "                    start_date = 'null'\n",
    "                    end_date = convert_date_to_dt(event_date.replace('closing ', '').replace(',', ''))\n",
    "                elif 'â€“' in event_date:\n",
    "                    dates = event_date.split('â€“')\n",
    "                    # If there are two commas, this implies that both dates have a month, day, and year\n",
    "                    if event_date.count(',') == 2:\n",
    "                        start_date = convert_date_to_dt(dates[0].replace(',', ''))\n",
    "                        end_date = convert_date_to_dt(dates[1].replace(',', ''))\n",
    "                    # If not, this implies either the day or year is missing\n",
    "                    elif event_date.count(',') == 1:\n",
    "                        if dates[1] == 'ongoing':\n",
    "                            start_date = convert_date_to_dt(dates[0].replace(',', ''))\n",
    "                            end_date = 'null'\n",
    "                        # The day or year is missing in one of the dates\n",
    "                        else:\n",
    "                            # The start date is missing something\n",
    "                            if len(dates[0].split()) == 2:\n",
    "                                # Has year, missing day\n",
    "                                if len(dates[0].split()[1]) == 4:\n",
    "                                    date_0_rev = dates[0].split()[0] + ' 1 ' + dates[0].split()[1]\n",
    "                                    start_date = convert_date_to_dt(date_0_rev)\n",
    "                                # Has day, missing year (use year from end date)\n",
    "                                else:\n",
    "                                    date_0_rev = dates[0].split()[0] + ' ' + dates[0].split()[1] + ' ' + dates[1].split()[-1]\n",
    "                                    start_date = convert_date_to_dt(date_0_rev.replace(',', ''))\n",
    "                                end_date = convert_date_to_dt(dates[1].replace(',', ''))\n",
    "                            # The end date is missing something\n",
    "                            else:\n",
    "                                # Has year, missing day\n",
    "                                if len(dates[1].split()[1]) == 4:\n",
    "                                    date_1_rev = dates[1].split()[0] + ' 1 ' + dates[1].split()[1]\n",
    "                                    end_date = convert_date_to_dt(date_1_rev)\n",
    "                                # Has day, missing year (use year from start date)\n",
    "                                else:\n",
    "                                    date_1_rev = dates[1].split()[0] + ' ' + dates[1].split()[1] + ' ' + dates[0].split()[-1]\n",
    "                                    end_date = convert_date_to_dt(date_1_rev.replace(',', ''))\n",
    "                                start_date = convert_date_to_dt(dates[0].replace(',', ''))\n",
    "                    else:\n",
    "                        print(f'No commas found in: {event_date}')\n",
    "                else:\n",
    "                    start_date = 'null'\n",
    "                    end_date = 'null'\n",
    "\n",
    "                event_image_url = event.find(\"img\", class_=\"exhibitionsgrid-wrapper-grid-item-image\")['src']\n",
    "\n",
    "                event_details = {\n",
    "                    'name': event_title,\n",
    "                    'venue': 'SFMOMA',\n",
    "                    'description': event_description,\n",
    "                    'tags': ['exhibition'] + [phase_dict['phase']],\n",
    "                    'phase': phase_dict['phase'],\n",
    "                    'dates': {'start': start_date, 'end': end_date},\n",
    "                    'links': [\n",
    "                        {\n",
    "                            'link': event_link,\n",
    "                            'description': 'Event Page'\n",
    "                        },\n",
    "                        {\n",
    "                            'link': event_image_url,\n",
    "                            'description': 'Image'\n",
    "                        },\n",
    "                    ],\n",
    "\n",
    "                }\n",
    "\n",
    "                process_event(event_details)\n",
    "\n",
    "        else:\n",
    "            print(f\"Events container not found for phase: {phase_dict['phase']}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Function that:\n",
    "        1. Saves a copy of existing data\n",
    "        2. Scrapes data from the de Young Museum & Legion of Honor\n",
    "        3. Scrapes data from SFMOMA\n",
    "        4. Saves data as json\n",
    "    \"\"\"\n",
    "    # Save a copy of existing json data\n",
    "    try:\n",
    "        copy_json_file('website/events_db.json', 'website/events_db_copy.json')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    venue = \"de Young & Legion of Honor\"\n",
    "    print(f\"Starting scrape for {venue}\")\n",
    "    scrape_de_young_and_legion_of_honor()\n",
    "    print(f\"Finished scrape for {venue}\")\n",
    "\n",
    "    venue = 'SFMOMA'\n",
    "    print(f\"Starting scrape for {venue}\")\n",
    "    scrape_sfmoma()\n",
    "    print(f\"Finished scrape for {venue}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b422ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
