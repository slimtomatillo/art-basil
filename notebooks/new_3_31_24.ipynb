{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf71b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from hashlib import md5\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "DB_FILE = 'events_db.json'\n",
    "EVENT_TAGS = ['audio', 'closing', 'exhibition', 'family', 'free', 'opening',\n",
    "              'performance', 'queer', 'symposium', 'talk', 'tour', 'virtual', 'workshop']\n",
    "\n",
    "# Create month to month number dict\n",
    "month_to_num_dict = {\n",
    "    'jan': 1,\n",
    "    'feb': 2,\n",
    "    'mar': 3,\n",
    "    'apr': 4,\n",
    "    'may': 5,\n",
    "    'jun': 6,\n",
    "    'jul': 7,\n",
    "    'aug': 8,\n",
    "    'sep': 9,\n",
    "    'oct': 10,\n",
    "    'nov': 11,\n",
    "    'dec': 12\n",
    "}\n",
    "\n",
    "def copy_json_file(source_file_path, destination_file_path):\n",
    "    \"\"\"\n",
    "    Function to take the source path of a json file and make\n",
    "    a copy of the json file to the destination_file_path.\n",
    "    \"\"\"\n",
    "    # Step 1: Open and read the JSON file\n",
    "    with open(source_file_path, 'r') as json_file:\n",
    "        # Load the JSON content into a Python data structure\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Step 2: Create or open the destination JSON file and write the data to it\n",
    "    with open(destination_file_path, 'w') as destination_json_file:\n",
    "        # Write the data to the destination file\n",
    "        json.dump(data, destination_json_file, indent=4)  # You can use indent for pretty formatting if desired\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_db():\n",
    "    \"\"\"Load the event database from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(DB_FILE, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def save_db(db):\n",
    "    \"\"\"Save the event database to a JSON file.\"\"\"\n",
    "    with open(DB_FILE, 'w') as file:\n",
    "        json.dump(db, file, indent=4, default=str)\n",
    "\n",
    "def fetch_and_parse(url):\n",
    "    \"\"\"Fetch content of a webpage and return a BeautifulSoup object.\"\"\"\n",
    "    try:\n",
    "        # Send a GET request to fetch the webpage content\n",
    "        response = requests.get(url, headers={'User-Agent': 'Your Bot 0.1'})\n",
    "        response.raise_for_status()\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_event_hash(event_details):\n",
    "    \"\"\"Generate a hash of the event details to detect changes.\"\"\"\n",
    "    event_string = json.dumps(event_details, sort_keys=True, default=str)\n",
    "    return md5(event_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "def generate_unique_identifier(event_details):\n",
    "    \"\"\"Generate a unique identifier for an event.\"\"\"\n",
    "    return f\"{event_details['name']}-{event_details['venue']}\"\n",
    "\n",
    "def process_event(event_details):\n",
    "    \"\"\"Check if an event is new or has changed and update the database accordingly.\"\"\"\n",
    "    db = load_db()\n",
    "    site_events = db.get(event_details['venue'], {})\n",
    "    \n",
    "    event_id = generate_unique_identifier(event_details)\n",
    "    event_hash = generate_event_hash(event_details)\n",
    "\n",
    "    if event_id not in site_events or site_events[event_id]['hash'] != event_hash:\n",
    "        print(f\"Updating event: {event_details['name']}\")\n",
    "        site_events[event_id] = {**event_details, 'hash': event_hash}\n",
    "        db[event_details['venue']] = site_events\n",
    "        save_db(db)\n",
    "    else:\n",
    "        print(f\"No changes detected for event: {event_details['name']}\")\n",
    "\n",
    "def scrape_de_young_and_legion_of_honor():\n",
    "    \"\"\"Scrape and process events from the de Young and Legion of Honor.\"\"\"\n",
    "    \n",
    "    def convert_date_to_dt(date_string):\n",
    "        \"\"\"Takes a date in string form and converts it to a dt object\"\"\"\n",
    "        global month_to_num_dict\n",
    "\n",
    "        date_parts = date_string.split()\n",
    "        month_num = int(month_to_num_dict[date_parts[0]])\n",
    "        day = int(date_parts[1])\n",
    "        year = int(date_parts[2])\n",
    "        if month_num and day and year:\n",
    "            date_dt = dt.date(year, month_num, day)\n",
    "        return date_dt\n",
    "\n",
    "    # Declare list of url dicts and then iterate through them\n",
    "    urls = [\n",
    "        {\n",
    "            'venue': 'de Young',\n",
    "            'base_url': 'https://www.famsf.org/calendar?type=exhibition&location=de-young'\n",
    "        },\n",
    "        {\n",
    "            'venue': 'Legion of Honor',\n",
    "            'base_url': 'https://www.famsf.org/calendar?type=exhibition&location=legion-of-honor'\n",
    "        },\n",
    "        {\n",
    "            'venue': 'Virtual',\n",
    "            'base_url': 'https://www.famsf.org/calendar?type=exhibition&location=virtual'\n",
    "        }\n",
    "    ]\n",
    "    for u in urls:\n",
    "        # Iterate through the pages\n",
    "        for i in range(1, 10):\n",
    "            if i == 1:\n",
    "                url = u['base_url']\n",
    "            else:\n",
    "                url = u['base_url'] + f\"?page={i}\"\n",
    "            soup = fetch_and_parse(url)\n",
    "            if soup:\n",
    "                # Find elements a class\n",
    "                group_elements = soup.find_all(class_=\"mt-24 xl:mt-32\")\n",
    "                \n",
    "                # If no pages left, exit loop\n",
    "                if len(group_elements) == 0: # this will be 0 when we've gone through all the pages\n",
    "                    break\n",
    "\n",
    "                for e in group_elements:\n",
    "\n",
    "                    # Extract name\n",
    "                    name = e.find(\"a\").find(\"h3\").get_text().strip()\n",
    "\n",
    "                    # Extract link\n",
    "                    link = e.find(\"a\").get(\"href\")\n",
    "\n",
    "                    # Extract date info\n",
    "                    date = e.find(class_=\"mt-12 text-secondary f-subheading-1\").get_text()\n",
    "                    \n",
    "                    # Identify phase and date fields\n",
    "                    if date.lower().split()[0] == 'through':\n",
    "                        # Get phase\n",
    "                        phase = 'Current'\n",
    "                        # Get dt versions of start and end dates\n",
    "                        start_date = np.nan\n",
    "                        dates = [date.lower().replace(',', '').replace('through ', '')]\n",
    "                        end_date = convert_date_to_dt(dates[0])\n",
    "\n",
    "                    else:\n",
    "                        # Get phase\n",
    "                        phase = 'Future'\n",
    "                        # Get dt versions of start and end dates\n",
    "                        dates = date.lower().replace(',', '').split(' â€“ ')\n",
    "                        # If no year in the date, add the year (use year of end date)\n",
    "                        if len(dates[0].split()) == 2:\n",
    "                            dates[0] = dates[0] + ' ' + dates[1].split()[-1]\n",
    "                        start_date = convert_date_to_dt(dates[0])\n",
    "                        end_date = convert_date_to_dt(dates[1])\n",
    "                                            \n",
    "                    event_details = {\n",
    "                        'name': name,\n",
    "                        'venue': u['venue'],\n",
    "                        'tags': ['exhibition'],\n",
    "                        'phase': phase, # Possible phases are past, current, future\n",
    "                        'dates': {'start': start_date, 'end': end_date},\n",
    "                        'links': [\n",
    "                            {\n",
    "                                'link': link,\n",
    "                                'description': 'Event Page'\n",
    "                            },\n",
    "                        ],\n",
    "                    }\n",
    "                    process_event(event_details)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Function that:\n",
    "        1. Saves a copy of existing data\n",
    "        2. Scrapes data from the de Young Museum & Legion of Honor\n",
    "        3. Scrapes data from the Berkeley Art Center\n",
    "        4. Saves data as json\n",
    "    \"\"\"\n",
    "    # Save a copy of existing json data\n",
    "    try:\n",
    "#         copy_json_file('website/data.json', 'website/data_copy.json')\n",
    "        copy_json_file('events_db.json', 'events_db_copy.json')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    venue = \"de Young & Legion of Honor\"\n",
    "    print(f\"Starting scrape for {venue}\")\n",
    "    scrape_de_young_and_legion_of_honor()\n",
    "    print(f\"Finished scrape for {venue}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ab3a48b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape for de Young & Legion of Honor\n",
      "No changes detected for event: Lhola Amira: Facing the Future\n",
      "No changes detected for event: Bouquets to Art 2024\n",
      "No changes detected for event: Lee Mingwei: Rituals of Care\n",
      "No changes detected for event: Irving Penn\n",
      "No changes detected for event: Fashioning San Francisco: A Century of Style\n",
      "No changes detected for event: Nampeyo and the SikyÃ¡tki Revival\n",
      "No changes detected for event: American Beauty: The Osher Collection of American Art\n",
      "No changes detected for event: Art and War in the Renaissance: The Battle of Pavia Tapestries\n",
      "No changes detected for event: Tamara de Lempicka\n",
      "No changes detected for event: Leilah Babirye: We Have a History\n",
      "No changes detected for event: Rose B. Simpson: LEXICON\n",
      "No changes detected for event: About Place: Bay Area Artists from the Svane Gift\n",
      "No changes detected for event: Embroidered Histories\n",
      "No changes detected for event: Contemporary Painting in Papua New Guinea: Mathias Kauage and His Family\n",
      "No changes detected for event: Drawing the Line: Michelangelo to Asawa\n",
      "No changes detected for event: Japanese Prints in Transition: From the Floating World to the Modern World\n",
      "No changes detected for event: Zuan-cho: Kimono Design in Modern Japan (1868â€“1912)\n",
      "No changes detected for event: Woodcut: Primary Printmaking\n",
      "No changes detected for event: Mary Cassatt at Work\n",
      "No changes detected for event: Dress Rehearsal: The Art of Theatrical Design\n",
      "Finished scrape for de Young & Legion of Honor\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797192f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
